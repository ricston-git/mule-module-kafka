/**
 * This file was automatically generated by the Mule Development Kit
 */
package org.ricston;

import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.util.Properties;

import kafka.api.FetchRequestBuilder;
import kafka.javaapi.FetchResponse;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.javaapi.message.ByteBufferMessageSet;
import kafka.message.MessageAndOffset;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.mule.api.ConnectionException;
import org.mule.api.annotations.Configurable;
import org.mule.api.annotations.Connector;
import org.mule.api.annotations.Processor;
import org.mule.api.annotations.Source;
import org.mule.api.annotations.param.ConnectionKey;
import org.mule.api.annotations.param.Default;
import org.mule.api.annotations.param.Optional;
import org.mule.api.callback.SourceCallback;

/**
 * Cloud Connector
 * 
 * @author MuleSoft, Inc.
 */
@Connector(name = "kafka", friendlyName = "kafka", minMuleVersion = "3.4")
public class KafkaConnector {
	private static Logger logger = LoggerFactory.getLogger(KafkaConnector.class);
	Properties props = new Properties();

	@Optional
	@Configurable
	String kafkaServerURL = "localhost";

	public Properties getProps() {
		return props;
	}

	public void setProps(Properties props) {
		this.props = props;
	}

	public String getKafkaServerURL() {
		return kafkaServerURL;
	}

	public void setKafkaServerURL(String kafkaServerURL) {
		this.kafkaServerURL = kafkaServerURL;
	}

	public Integer getKafkaServerPort() {
		return kafkaServerPort;
	}

	public void setKafkaServerPort(Integer kafkaServerPort) {
		this.kafkaServerPort = kafkaServerPort;
	}

	public Integer getConnectionTimeOut() {
		return connectionTimeOut;
	}

	public void setConnectionTimeOut(Integer connectionTimeOut) {
		this.connectionTimeOut = connectionTimeOut;
	}

	public Integer getReconnectInterval() {
		return reconnectInterval;
	}

	public void setReconnectInterval(Integer reconnectInterval) {
		this.reconnectInterval = reconnectInterval;
	}

	public Integer getKafkaProducerBufferSize() {
		return kafkaProducerBufferSize;
	}

	public void setKafkaProducerBufferSize(Integer kafkaProducerBufferSize) {
		this.kafkaProducerBufferSize = kafkaProducerBufferSize;
	}

	public String getClientId() {
		return clientId;
	}

	public void setClientId(String clientId) {
		this.clientId = clientId;
	}

	@Optional
	@Configurable
	Integer kafkaServerPort = 9092;

	@Optional
	@Configurable
	Integer connectionTimeOut = 100000;

	@Optional
	@Configurable
	Integer reconnectInterval = 10000;

	@Optional
	@Configurable
	Integer kafkaProducerBufferSize = 64 * 1024;

	@Configurable
	String clientId = "muleKafkaClient";

	/**
	 * consume messages from kafka
	 * 
	 * @param callback
	 * @param topic
	 */
	@Source(name = "consumer", friendlyName = "consumer")
	public void consumer(SourceCallback callback, String topic) {
		Long pollPeriod = new Long(10000);
		SimpleConsumer simpleConsumer = new SimpleConsumer(kafkaServerURL, kafkaServerPort, connectionTimeOut,
				kafkaProducerBufferSize, clientId);
		while (!Thread.interrupted()) {
			kafka.api.FetchRequest req = new FetchRequestBuilder().clientId(clientId).addFetch(topic, 0, 0L, 100)
					.build();
			System.out.println("Testing single fetch 2");
			FetchResponse fetchResponse = simpleConsumer.fetch(req);
			try {
				long numRead = 0;
				for (MessageAndOffset messageAndOffset : fetchResponse.messageSet(topic, a_partition)) {
					long currentOffset = messageAndOffset.offset();
					if (currentOffset < readOffset) {
						System.out.println("Found an old offset: " + currentOffset + " Expecting: " + readOffset);
						continue;
					}
					readOffset = messageAndOffset.nextOffset();
					ByteBuffer payload = messageAndOffset.message().payload();

					byte[] bytes = new byte[payload.limit()];
					payload.get(bytes);
					System.out.println(String.valueOf(messageAndOffset.offset()) + ": " + new String(bytes, "UTF-8"));
					numRead++;
					a_maxReads--;
				}

				if (numRead == 0) {
					System.err.println("sleeping .. ");
					Thread.sleep(pollPeriod);
					continue;
				}

				callback.process(getMessage(msg));

			} catch (InterruptedException e) {
				logger.error(e.getMessage(), e);
			} catch (Exception e) {
				e.printStackTrace();
			}
		}

	}

	@Processor(name = "send")
	public void send(String topic, String message) {
		System.err.println("Sending message ... ");
		props.put("serializer.class", "kafka.serializer.StringEncoder");
		props.put("metadata.broker.list", "localhost:9092");
		// Use random partitioner. Don't need the key type. Just set it to
		// Integer.
		// The message is of type String.
		kafka.javaapi.producer.Producer producer = new kafka.javaapi.producer.Producer<Integer, String>(
				new ProducerConfig(props));
		producer.send(new KeyedMessage<Integer, String>(topic, message));

	}

	// @ValidateConnection
	public boolean isConnected() {
		return true; // need to validate how to ensure kakfa is up?
	}

	// @Connect
	public void connect(@ConnectionKey String accessKey, String secretKey, String queueName) throws ConnectionException {
		// connect to the kafka server based on the connection properties ..
		// perhaps set isConnected variable here
	}

	// @Disconnect
	public void disconnect() {
		// how to disconnect from kafka ?? dispose consumer?
	}

	// @Todo something up there how messages are buffered??
	private static String getMessage(ByteBufferMessageSet messageSet) {
		try {
			for (MessageAndOffset messageAndOffset : messageSet) {
				ByteBuffer payload = messageAndOffset.message().payload();
				byte[] bytes = new byte[payload.limit()];
				payload.get(bytes);
				return new String(bytes, "UTF-8");
			}
		} catch (UnsupportedEncodingException e) {
		}
		return "";
	}

}
